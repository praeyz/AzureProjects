# AzureDataFactory
I plan to configure source control in the Data Factory and commit the changes here. 

# Implementing a Partition Strategy for Azure Data Factory Pipelines

## Introduction

completed a hands-on lab on implementing a partition strategy for Azure Data Factory pipelines. This lab provided me with practical experience in provisioning an Azure Data Factory workspace, creating pipeline activities, and migrating data from an Azure SQL Database into a data lake. Additionally, I learned how to create partitions during the data migration process.

## Lab Objectives

1. **Prepare the Environment:** Set up the necessary Azure resources, including an Azure Data Factory workspace and the data sources required for the lab.
2. **Create Data Sources:** Configure data sources and sinks for data migration.
3. **Create a Data Flow:** Design a data flow to define how data is transformed and partitioned during the migration process.
4. **Create a Data Pipeline:** Implement a data pipeline to orchestrate the data migration and partitioning process.

## Steps and Process

1. **Provision Azure Data Factory Workspace:**
   - Set up an Azure Data Factory workspace to manage and orchestrate data migration and transformation activities.
   
2. **Configure Data Sources:**
   - Define the source (Azure SQL Database) and the destination (Data Lake) for data migration.
   - Set up connections and integration datasets.

3. **Design Data Flow:**
   - Create a data flow to map and transform data during migration.
   - Implement partitioning logic to ensure data is efficiently organized and accessible.

4. **Build Data Pipeline:**
   - Develop a data pipeline to automate the data migration process.
   - Include activities for data extraction, transformation, and loading (ETL).

## Conclusion
This lab gave me a comprehensive understanding of implementing a partition strategy in Azure Data Factory pipelines, specifically using 5 hash partitions. I gained hands-on experience in provisioning Azure resources, designing data flows, and building data pipelines.
